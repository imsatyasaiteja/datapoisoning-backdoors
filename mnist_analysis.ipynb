{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "91734af9",
            "metadata": {},
            "source": [
                "### MNIST Backdoor Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "df4282a6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Drive (Colab)\n",
                "from google.colab import drive\n",
                "drive.mount(\"/content/drive\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9a1accbf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports and device\n",
                "import argparse\n",
                "import math\n",
                "import random\n",
                "from pathlib import Path\n",
                "from typing import Tuple\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "from torch import nn\n",
                "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "23d3492e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths (defaults mirror original TODO notebook on Drive)\n",
                "dataset_path = \"/content/drive/MyDrive/Assignment_2_files_updated/mnist_test_data.pt\"\n",
                "weights_path = \"/content/drive/MyDrive/Assignment_2_files_updated/model_weights_poisoned_partC.pth\"\n",
                "poison_fraction = 0.05\n",
                "target_label = 0\n",
                "trigger_size = 3\n",
                "opt_steps = 300\n",
                "opt_lr = 0.003\n",
                "batch_size = 128\n",
                "seed = 71"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0eaab246",
            "metadata": {},
            "outputs": [],
            "source": [
                "# CNN classifier\n",
                "class CNN(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
                "            nn.ReLU(),\n",
                "            nn.MaxPool2d(2, 2),\n",
                "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
                "            nn.ReLU(),\n",
                "            nn.MaxPool2d(2, 2),\n",
                "            nn.Flatten(),\n",
                "            nn.Linear(64 * 7 * 7, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, 10),\n",
                "        )\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        return self.net(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "386cbabc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data loading helper\n",
                "def load_mnist_tensor(path: str, batch_size: int):\n",
                "    dataset = torch.load(path)\n",
                "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
                "    return dataset, loader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5401a13d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Trigger utilities\n",
                "def corner_trigger(trigger_size: int, channels: int = 1, value: float = 0.85) -> torch.Tensor:\n",
                "    trigger = torch.zeros((channels, trigger_size, trigger_size))\n",
                "    trigger.fill_(value)\n",
                "    return trigger\n",
                "\n",
                "def apply_trigger(images: torch.Tensor, trigger_patch: torch.Tensor) -> torch.Tensor:\n",
                "    patched = images.clone()\n",
                "    h, w = trigger_patch.shape[-2:]\n",
                "    patched[:, :, -h:, -w:] = trigger_patch.to(images.device)\n",
                "    return patched"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "04f980a5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset with fractional poisoning (backdoor or label flip)\n",
                "class FractionalBackdoorDataset(Dataset):\n",
                "    def __init__(self, base_dataset: Dataset, trigger_patch: torch.Tensor, target_label: int, poison_fraction: float, mode: str = \"backdoor\", seed: int = 71):\n",
                "        self.base_dataset = base_dataset\n",
                "        self.trigger_patch = trigger_patch\n",
                "        self.target_label = target_label\n",
                "        self.mode = mode\n",
                "        if not 0 < poison_fraction <= 1:\n",
                "            raise ValueError(\"poison_fraction must be in (0,1].\")\n",
                "        rng = np.random.default_rng(seed)\n",
                "        total = len(base_dataset)\n",
                "        poison_count = max(1, int(total * poison_fraction))\n",
                "        self.poison_indices = set(rng.choice(total, size=poison_count, replace=False).tolist())\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.base_dataset)\n",
                "\n",
                "    def __getitem__(self, idx: int):\n",
                "        img, label = self.base_dataset[idx]\n",
                "        if idx in self.poison_indices:\n",
                "            if self.mode == \"backdoor\":\n",
                "                img = apply_trigger(img.unsqueeze(0), self.trigger_patch).squeeze(0)\n",
                "                label = self.target_label\n",
                "            elif self.mode == \"label_flip\":\n",
                "                candidates = list(range(10))\n",
                "                candidates.remove(int(label))\n",
                "                label = random.choice(candidates)\n",
                "            else:\n",
                "                raise ValueError(f\"Unsupported mode: {self.mode}\")\n",
                "        return img, label"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dbb038aa",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluation metrics\n",
                "@torch.no_grad()\n",
                "def accuracy(model: nn.Module, loader: DataLoader) -> float:\n",
                "    model.eval()\n",
                "    total = 0; correct = 0\n",
                "    for images, labels in loader:\n",
                "        images = images.to(device)\n",
                "        labels = labels.to(device)\n",
                "        preds = model(images).argmax(1)\n",
                "        total += labels.size(0)\n",
                "        correct += (preds == labels).sum().item()\n",
                "    return 100.0 * correct / total\n",
                "\n",
                "@torch.no_grad()\n",
                "def attack_success_rate(model: nn.Module, loader: DataLoader, target_label: int) -> float:\n",
                "    model.eval()\n",
                "    total = 0; success = 0\n",
                "    for images, _ in loader:\n",
                "        images = images.to(device)\n",
                "        preds = model(images).argmax(1)\n",
                "        total += preds.size(0)\n",
                "        success += (preds == target_label).sum().item()\n",
                "    return 100.0 * success / total"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8c82a949",
            "metadata": {},
            "outputs": [],
            "source": [
                "# White-box trigger optimization\n",
                "def optimize_trigger_for_asr(model: nn.Module, base_dataset: Dataset, target_label: int, trigger_size: int, steps: int, lr: float, batch_size: int) -> torch.Tensor:\n",
                "    model.eval()\n",
                "    loader = DataLoader(base_dataset, batch_size=batch_size, shuffle=False)\n",
                "    trigger = torch.randn((1, 1, trigger_size, trigger_size), device=device, requires_grad=True)\n",
                "    optimizer = torch.optim.Adam([trigger], lr=lr)\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    for step in range(steps):\n",
                "        optimizer.zero_grad()\n",
                "        all_images = []\n",
                "        for images, _ in loader:\n",
                "            images = images.to(device)\n",
                "            all_images.append(apply_trigger(images, trigger))\n",
                "        all_images = torch.cat(all_images, dim=0)\n",
                "        labels = torch.full((len(all_images),), target_label, dtype=torch.long, device=device)\n",
                "        loss = criterion(model(all_images), labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        if step % max(1, steps // 5) == 0:\n",
                "            temp_loader = DataLoader(TensorDataset(all_images.detach(), labels), batch_size=batch_size)\n",
                "            asr = attack_success_rate(model, temp_loader, target_label)\n",
                "            print(f\"[white-box] step={step} loss={loss.item():.4f} asr={asr:.2f}%\")\n",
                "    return trigger.detach()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8267966d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample-complexity bound\n",
                "def detection_sample_complexity(gap: float, delta: float = 0.05) -> int:\n",
                "    if gap <= 0:\n",
                "        return math.inf\n",
                "    return math.ceil(math.log(2 / delta) / (2 * gap * gap))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e94d4e0c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run experiment with defaults (matches original TODO paths)\n",
                "test_dataset, clean_loader = load_mnist_tensor(dataset_path, batch_size)\n",
                "model = CNN().to(device)\n",
                "state = torch.load(weights_path, map_location=device)\n",
                "model.load_state_dict(state)\n",
                "model.eval()\n",
                "clean_acc = accuracy(model, clean_loader)\n",
                "print(f\"Clean accuracy: {clean_acc:.2f}%\")\n",
                "base_trigger = corner_trigger(trigger_size)\n",
                "poisoned_bb = FractionalBackdoorDataset(test_dataset, base_trigger, target_label=target_label, poison_fraction=poison_fraction, mode=\"backdoor\", seed=seed)\n",
                "bb_loader = DataLoader(poisoned_bb, batch_size=batch_size, shuffle=False)\n",
                "bb_asr = attack_success_rate(model, bb_loader, target_label)\n",
                "print(f\"Black-box ASR (fixed trigger, {poison_fraction*100:.1f}% poison): {bb_asr:.2f}%\")\n",
                "white_trigger = optimize_trigger_for_asr(model, test_dataset, target_label=target_label, trigger_size=trigger_size, steps=opt_steps, lr=opt_lr, batch_size=batch_size)\n",
                "poisoned_wb = FractionalBackdoorDataset(test_dataset, white_trigger.cpu(), target_label=target_label, poison_fraction=poison_fraction, mode=\"backdoor\", seed=seed)\n",
                "wb_loader = DataLoader(poisoned_wb, batch_size=batch_size, shuffle=False)\n",
                "wb_asr = attack_success_rate(model, wb_loader, target_label)\n",
                "print(f\"White-box ASR (optimized trigger, {poison_fraction*100:.1f}% poison): {wb_asr:.2f}%\")\n",
                "label_flip_ds = FractionalBackdoorDataset(test_dataset, trigger_patch=base_trigger, target_label=target_label, poison_fraction=poison_fraction, mode=\"label_flip\", seed=seed)\n",
                "label_flip_loader = DataLoader(label_flip_ds, batch_size=batch_size, shuffle=False)\n",
                "label_flip_acc = accuracy(model, label_flip_loader)\n",
                "print(f\"Label-flip accuracy (untargeted, {poison_fraction*100:.1f}% poison): {label_flip_acc:.2f}%\")\n",
                "bb_gap = abs(bb_asr / 100.0 - clean_acc / 100.0)\n",
                "wb_gap = abs(wb_asr / 100.0 - clean_acc / 100.0)\n",
                "print(f\"Samples to distinguish clean vs. black-box (delta=0.05): {detection_sample_complexity(bb_gap)}\")\n",
                "print(f\"Samples to distinguish clean vs. white-box (delta=0.05): {detection_sample_complexity(wb_gap)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
